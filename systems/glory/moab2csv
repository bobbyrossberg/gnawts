#!/usr/bin/perl

# seed like this
#  nice grep -h JOBSTART /admin/splunk/moab/stats/events.* | grep rs | sort -n -k 2 >new &
#  ./moab2csv -new
#
# update with a cron job like:
# 3,8,13,18,23,28,33,38,43,48,53,58 * * * * cd /admin/splunk/current/etc/system/lookups; nice ./moab2csv

$splunkdir = "/logs/splunk";

$file = "$splunkdir/etc/apps/hpc/lookups/jobs.csv";
open OUTFILE, ">>$file" or die "can't open $file: !$\n";
if (! -s $file) {
	$when = "earliest=-10d@d";
	print OUTFILE "start,host,user,job,jobt\n";
}
else {
	$lastline = `tail -1 $file`;
	($lasttime) = split /,/, $lastline;
	$when = "_time>$lasttime";
}

$cmd = "$splunkdir/bin/splunk search 'sourcetype=moabs etype=jobstart $when' -maxout 9999999999 -auth admin:glory101";
open INEVENTS, "$cmd |" or die "can't run splunk\n";

while(<INEVENTS>) {
	reset 'h';
	@F = split;
	$job = $F[3];
	$user = $F[7];
	$start = $F[14];
	next if ($start == $time);  # i think this is needed to avoid duplicates, prior to splunk 4.1.4
	$hosts = $F[41];
	@hosts = split /,/, $hosts;

	# remove dupes
	map {$hosts{$_}++} @hosts;
	@hosts = sort keys %hosts;

	foreach $host (@hosts) {
		$newjobs .= "$start,$host,$user,$job,$job.$start\n"
	}
}
close INEVENTS;

print OUTFILE $newjobs;
close OUTFILE;
